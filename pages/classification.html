<body>
  <h4>Method</h4>
  <p> Classifying movie genres using movie plot summaries examines the power of phrase mining and explores its
    application in information extraction in movie sector. The results from the classification can also disclose some
    implicit features of movie genres through important phrases.
  </p>
  <img src='assets/images/top10 genres.png' width="450">
  <p>
    To obtain ideal results, we construct a baseline model:
  <ol>
    <li>Using TF-IDF vectorizer for words embedding</li>
    <li>Construct multiple logistic regressions inside OneVsRest Classifier for each genre to predict the label. </li>
    <li>Evaluating the model use f-1 score. We also use some costumed indicator - percentage of correctly predicted
      label and percentage of movie that has at least one correctly predicted label</li>
  </ol>
  </p>
  <p>We further improve our model in following ways:
  <ol>
    <li>We try out different algorithms in OneVsRest Classifier including LinearSVC and Multi-linear Perceptron
      classifier. </li>
    <li>We perform GridSearch for hyper-parameters tuning, changing different loss function and different regularization
      penalty for best performance.</li>
  </ol>
  We also construct the text feature in two ways
  <ol>
    <li> Use all non-stopwords in plot summaries as text feature</li>
    <li>Use only phrases generated by Autophrase as feature</li>

  </ol>
  We can compare the results from these two approaches to explore the contribution of phrases to the model.
  </p>
  <p>
    Finally, to better evaluate our model, we will interpret the coefficients of words of classifiers to see what words
    or phrases that our predictions mostly rely on. We then generate plots for the words/phrases rank to evaluate the
    model using our understanding of genre and see if the result can tell us anything more about the genre that we do
    not know before.
  </p>
  <h4>Results</h4>
  <p>
    Our baseline had a result F-1 score 0.332 using whole summaries as feature and 0.29 using phrases. Our final model
    had a result F-1 score of 0.407 using summaries and 0.364 using phrases.
  </p>
  <table class="variables">
    <thead>
      <tr>
        <td>Model</td>
        <td>Feature</td>
        <td>F-1 Score</td>
        <td>label %</td>
        <td>movie %</td>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>baseline</td>
        <td>summary text</td>
        <td>0.33</td>
        <td>0.043</td>
        <td>0.12</td>
      </tr>
      <tr>
        <td>baseline</td>
        <td>phrases</td>
        <td>0.29</td>
        <td>0.038</td>
        <td>0.11</td>
      </tr>
      <tr>
        <td>final model</td>
        <td>summary text</td>
        <td>0.41</td>
        <td>0.062</td>
        <td>0.13</td>
      </tr>
      <tr>
        <td>final model</td>
        <td>phrases</td>
        <td>0.36</td>
        <td>0.053</td>
        <td>0.13</td>
      </tr>

    </tbody>
  </table>
  <p>
    We extract the coefficients of words from the classifiers to understand what words matter most for each genre in our
    classification. It helps to evaluate the model as well as deliver some insights on genre prediction. The plot shows
    the top 20 words that has the largest coefficients, thus the highest significance to the that genre. Due to lacking
    of ground truth to compare with, we will put forward some interesting findings from Figure
    6 and Figure 7 :
  </p>
  <img src='assets\images\final model - phrases.png' width = 700>
  <img src='assets/images\final model - summary text.png' width = 700>
  <p>
  <ul>
    <li><b>Drama:</b> Using summary text, we found some female names like Helene, Carmen, Johann in the plots. Our model
      seems to place some importance in the Female name when predicting Drama movies. While using phrases, we found more
      animals in the top phrases, like goa, cattle, guinea pig, etc. There are more negative words/phrases as well, like
      exorcism, flatulence, local-mob, domestic violence etc. </li>
    <li><b>Comedy:</b> Using summary text, interestingly, we found some male names in the top words, like Elmo, Davey,
      Jones, Doug etc. Other words mostly human behaviors or status like hurry, ashamed, dig, chop, etc. Using phrase,
      we find the phrases and words are more region related, like Ethiopian, Baltimore, British Rag etc. There are also
      lots of hostile words like hostage, homophobia, ensuing battle, blockade, destructive, etc. One hypothesis is that
      lots of comedies use contradictions to provoke funny scenes. </li>
    <li><b>Romance:</b> Using summary text, we still see lots of people names. There are also words describing relation
      status like deserted and attracted. Using phrases, there appear to be some locations and community like Havana,
      Houston, Rojo, death squad, and firing squad. The results here seem quite deviated from what we expected.</li>
    <li><b>Thriller:</b> Using summary text, there are words with uncanny meanings like alien, immortality, bury, deadly.
      Using phrases, there are words related to criminal scenes like drug cartel, captured and imprisoned.</li>
  </ul>
  </p>

</body>