<body>
  <p>
    We build a clustering pipeline on the movie plot summaries to discover
    relationships between movies in a marginally supervised perspective.
  </p>
  <h3>Method</h3>
  <p>
    As shown in the figure below, our clustering pipeline contains three major
    components: (1) build and fine-tune a sentence transformer to acquire
    document embeddings, (2) pick representative sentences to condense plot
    summaries, and (3) cluster document embeddings.
  </p>

  <img src="assets/images/clustering_pipeline.png" width="90%" />

  <h4>Model</h4>
  <p>
    We purpose to estimate the document embeddings based on a pre-trained
    <a href="https://www.sbert.net/">sentence transformer</a>. Experiments in
    <a href="https://arxiv.org/abs/2004.02105">Aharoni and Goldberg (2020)</a>
    show that the contextualized embeddings generated by pre-trained language
    models have the capability to preserve domain information. Compared with
    vanilla transformer networks, sentence transformers are further tailored to
    estimate contextualized sentence embeddings for semantic similarity tasks
    (<a href="https://arxiv.org/abs/1908.10084">Reimers and Gurevych, 2019</a>).
    Therefore, the sentence transformer is the strongest out-of-the-box model we
    can utilize. Since the sentence transformer expects sentences as input, we
    split each summary into sentences, acquire an embedding for each sentence,
    and average the embeddings to form document embeddings.
  </p>

  <h4>Condense plot summaries</h4>

  <img src="assets/images/sents_per_doc.png" width="60%" />

  <p>
    As shown in the figure above, the distribution of the number of sentences in
    each movie summary is a long-tailed distribution, where 46.05% of summaries
    have more than 10 sentences. Since the transformer truncates the document to
    a fixed length, the embeddings acquired from the model cannot fully capture
    the semantics of long summaries. Therefore, it is necessary to condense the
    movie summaries while preserving most of the gist. We purpose to rank the
    importance of the sentences and choose sentences with top-<i>k</i>
    importance to represent the entire summary. Here, we define the importance
    as the average sublinear TF-IDF score, where the dictionary of terms is only
    the quality phrases. Under this notion of importance, sentences with no
    quality phrases have an importance score of 0, and sentences with more
    uncommon quality phrases have a higher importance score. If the document has
    fewer than <i>k</i> sentences, we keep all sentences. If the document
    contains no quality phrases, we sample <i>k</i>
    sentences randomly. To acquire the document embeddings, we only use
    representative sentences instead of all sentences.
  </p>

  <h4>Fine-tuning</h4>
  <p>
    Fine-tuning a pre-trained language model with a specific downstream task is
    a common practice to enhance the model performance. In our clustering
    pipeline, we use semantic similarity as the downstream task, since we expect
    documents with similar meanings to have closer embeddings, and vice versa.
    For two pairs of sentences, we determine the similarity by finding whether
    the genres associated with these sentences have overlap. To generate the
    training dataset, we iteratively sample sentence pairs until the training
    dataset contains at least <i>n</i> pairs, where half of the pairs have
    positive labels and the other half have negative labels. In each iteration,
    the algorithm samples two documents from the dataset, generates the binary
    label based on their genres, and selects <i>k</i> sentence pairs from two
    documents. We use the Contrastive Loss (<a
      href="https://ieeexplore.ieee.org/document/1640964"
      >Hadsell et al., 2006</a
    >) as the fine-tune target, since it aims to decrease the distance between
    sentence embeddings of similar sentence pairs and increase the distance
    between sentence embeddings of dissimilar sentence pairs.
  </p>

  <h4>Cluster embeddings</h4>
  <p>
    Following the practices in
    <a href="https://arxiv.org/abs/2004.02105">Aharoni and Goldberg (2020)</a>,
    we first reduce the dimensions of embeddings to 50 with PCA, then cluster
    the dimension-reduced embeddings using Gaussian Mixture Model (GMM).
    <a href="https://arxiv.org/abs/2004.02105">Aharoni and Goldberg (2020)</a>
    explained that the mixture model is more suitable since we can view each
    document embedding as from a mixture of different domain distributions, and
    applying PCA to the embeddings accelerates the training process while
    marginally damaging the performance.
  </p>

  <h3>Result</h3>

  <p>
    To examine our clustering pipeline, we cluster all vectors into 200 clusters
    and perform analysis on the output. We choose this number of clusters as the
    target since each cluster will roughly contain 200 movies, which is a
    reasonable size to derive fine-grained semantic units. Since the original
    dataset contains more than 300 genres and each document contains multiple
    genres, assigning the cluster label with the most common label within the
    cluster (<a href="https://arxiv.org/abs/2004.02105"
      >Aharoni and Goldberg, 2020</a
    >) is not feasible. Instead, we choose to determine the semantics of the
    cluster using multiple heuristics, including quality phrase distributions
    and the overlaps with existing genres.
  </p>

  <p>
    <b>Quality phrases.</b> To preserve the generality, we remove all phrases
    that occur in less than 100 documents. Then, for each phrase in each
    cluster, we calculate the document frequency within the cluster relative to
    its document frequency in the entire dataset. In other words, when a phrase
    occurs more frequently in this cluster than other clusters, we consider this
    phrase as a representative of this cluster. We rank the relative document
    frequencies of quality phrases in each cluster and pick the top 5 phrases to
    represent the cluster.
  </p>

  <p>
    The table below shows some examples of the representative quality phrases.
    From the examples, we can observe that many clusters can capture meanings
    that can be easily identified. We can infer that example 1 contains movies
    about special forces, example 2 is about martial arts in Asia, and example 3
    is about sports. However, we cannot identify the specific topic for some
    clusters like examples 4 and 5.
  </p>

  <table class="variables">
    <thead>
      <tr>
        <td>Example</td>
        <td>Quality Phrases</td>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1</td>
        <td>cia, sniper, helicopter, swat, laser, ...</td>
      </tr>
      <tr>
        <td>2</td>
        <td>kung fu, martial arts, monks, shanghai, thailand, ...</td>
      </tr>
      <tr>
        <td>3</td>
        <td>championship, basketball, coach, academic, baseball, ...</td>
      </tr>
      <tr>
        <td>4</td>
        <td>publisher, tokyo, province, economy, fever, ...</td>
      </tr>
      <tr>
        <td>5</td>
        <td>santa claus, fairy, frog, daffy, porky, ...</td>
      </tr>
    </tbody>
  </table>

  <p>
    <b>Genres.</b> We repeat the same process as examining the quality phrase
    memberships to examine the overlaps with genres. The table below shows some
    examples of the genre distributions. We can observe that many clusters (like
    example 1, 2, 3) gather the movies with similar genres together with only a
    few hints about similarity during the fine-tuning process. However, for
    clusters like examples 4 and 5, we cannot identify an obvious topic.
  </p>

  <table class="variables">
    <thead>
      <tr>
        <td>Example</td>
        <td>Genres</td>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1</td>
        <td>
          Slapstick, Sex comedy, Musical comedy, Comedy of manners, Comedy of
          Errors, ...
        </td>
      </tr>
      <tr>
        <td>2</td>
        <td>
          Courtroom Drama, Docudrama, Erotic Drama, Melodrama, Erotic thriller,
          ...
        </td>
      </tr>
      <tr>
        <td>3</td>
        <td>
          Stop motion, Children's Fantasy, Computer Animation, Animation,
          Family-Oriented Adventure, ...
        </td>
      </tr>
      <tr>
        <td>4</td>
        <td>Musical Drama, Ensemble, Experimental, Biography, Gay, ...</td>
      </tr>
      <tr>
        <td>5</td>
        <td>Comedy of manners, Domestic Comedy, Anime, Sports, Teen, ...</td>
      </tr>
    </tbody>
  </table>

  <p>
    We can see that the document embeddings can capture the semantics of the
    summaries, and feeding them to a clustering pipeline give us insights about
    the relationships between movies. By defining heuristics and summarize from
    each cluster, we can observe more connections that are hidden from the
    existing human crafted features.
  </p>
</body>
